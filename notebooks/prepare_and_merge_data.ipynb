{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f939c08-3f85-435a-b9be-f3b1027d4722",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color: #FFFFFF; font-family: Regular 400; background-color: #8B4000; padding: 10px; border-radius: 5px;\">\n",
    " Download existing ASR data from community except kallama: kudo galsen AI\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f6560-cec9-4435-b71c-fc8a6937f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================== #\n",
    "#     CONFIGURATION              #\n",
    "# ============================== #\n",
    "\n",
    "dataset_name = \"galsenai/wolof-audio-data\"\n",
    "output_dir = \"audio_galsenai\"\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "dev_dir = os.path.join(output_dir, \"dev\")\n",
    "\n",
    "\n",
    "EXCLUDED_SOURCES = ['kallama']  \n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(dev_dir, exist_ok=True)\n",
    "\n",
    "print(f\" Created output directories:\")\n",
    "print(f\"  - {train_dir}\")\n",
    "print(f\"  - {dev_dir}\")\n",
    "\n",
    "# ============================== #\n",
    "#     LOAD DATASET               #\n",
    "# ============================== #\n",
    "\n",
    "print(f\"\\n Loading dataset '{dataset_name}'...\")\n",
    "dataset = load_dataset(dataset_name)\n",
    "print(f\" Loaded dataset with splits: {list(dataset.keys())}\")\n",
    "\n",
    "\n",
    "# ============================== #\n",
    "#     FILTER OUT EXCLUDED SOURCES #\n",
    "# ============================== #\n",
    "\n",
    "print(f\"\\n Filtering out sources: {EXCLUDED_SOURCES}\")\n",
    "\n",
    "# Count before filtering\n",
    "train_before = len(dataset['train'])\n",
    "test_before = len(dataset['test'])\n",
    "\n",
    "# Filter both splits\n",
    "dataset['train'] = dataset['train'].filter(\n",
    "    lambda x: x['source'] not in EXCLUDED_SOURCES\n",
    ")\n",
    "dataset['test'] = dataset['test'].filter(\n",
    "    lambda x: x['source'] not in EXCLUDED_SOURCES\n",
    ")\n",
    "\n",
    "train_after = len(dataset['train'])\n",
    "test_after = len(dataset['test'])\n",
    "\n",
    "print(f\" Filtering complete:\")\n",
    "print(f\"   Train: {train_before} ‚Üí {train_after} ({train_before - train_after} removed)\")\n",
    "print(f\"   Test:  {test_before} ‚Üí {test_after} ({test_before - test_after} removed)\")\n",
    "print(f\"   Total removed: {(train_before - train_after) + (test_before - test_after)} files\")\n",
    "\n",
    "# Show remaining sources\n",
    "from collections import Counter\n",
    "train_sources = Counter(example['source'] for example in dataset['train'])\n",
    "test_sources = Counter(example['source'] for example in dataset['test'])\n",
    "\n",
    "print(f\"\\n Remaining sources in train split:\")\n",
    "for source, count in train_sources.most_common():\n",
    "    print(f\"   {source}: {count} files\")\n",
    "\n",
    "print(f\"\\n Remaining sources in test split:\")\n",
    "for source, count in test_sources.most_common():\n",
    "    print(f\"   {source}: {count} files\")\n",
    "\n",
    "# ============================== #\n",
    "#     SAVE FUNCTION              #\n",
    "# ============================== #\n",
    "\n",
    "def save_split_to_folder(split_name, output_folder):\n",
    "    \"\"\"\n",
    "    Saves audio from a dataset split to a local folder AND creates metadata files.\n",
    "    \n",
    "    Creates:\n",
    "    - Audio files (.wav)\n",
    "    - metadata.csv (filename, transcription, duration, etc.)\n",
    "    - metadata.jsonl (one JSON object per line)\n",
    "    \"\"\"\n",
    "    print(f\"\\n Processing '{split_name}' split...\")\n",
    "    split_data = dataset[split_name]\n",
    "    \n",
    "    saved_count = 0\n",
    "    error_count = 0\n",
    "    metadata_records = []\n",
    "    \n",
    "    # Metadata CSV file\n",
    "    csv_path = os.path.join(output_folder, \"metadata.csv\")\n",
    " \n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    for i, example in enumerate(tqdm(split_data, desc=f\"Saving {split_name}\")):\n",
    "        try:\n",
    "            # Extract audio data\n",
    "            audio_array = example[\"audio\"][\"array\"]\n",
    "            sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "            \n",
    "            # Get transcription - field is called 'sentence' in this dataset\n",
    "            transcription = example.get('sentence', '')\n",
    "            \n",
    "            # Get source field\n",
    "            source = example.get('source', '')\n",
    "            \n",
    "            # Calculate duration\n",
    "            duration = len(audio_array) / sampling_rate\n",
    "            \n",
    "            # Generate filename\n",
    "            if 'path' in example[\"audio\"] and example[\"audio\"][\"path\"]:\n",
    "                original_filename = os.path.basename(example[\"audio\"][\"path\"])\n",
    "                base_name, ext = os.path.splitext(original_filename)\n",
    "                if not ext or ext.lower() not in ['.wav', '.flac', '.ogg']:\n",
    "                    ext = '.wav'\n",
    "                filename = f\"{base_name}{ext}\"\n",
    "            else:\n",
    "                filename = f\"audio_{i:06d}.wav\"\n",
    "            \n",
    "            # Full file path\n",
    "            file_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # Ensure the parent directory exists\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            \n",
    "            # Save the audio file\n",
    "            sf.write(file_path, audio_array, sampling_rate)\n",
    "            saved_count += 1\n",
    "            \n",
    "            # Store metadata\n",
    "            metadata_record = {\n",
    "                'filename': filename,\n",
    "                'sentence': transcription,  # Using 'sentence' to match HuggingFace\n",
    "                'transcription': transcription,  # Also keep as 'transcription' for clarity\n",
    "                'source': source,\n",
    "                'duration': round(duration, 3),\n",
    "                'sampling_rate': sampling_rate,\n",
    "            }\n",
    "            \n",
    "            # Add any other fields from the example\n",
    "            for key, value in example.items():\n",
    "                if key not in ['audio', 'filename', 'sentence', 'transcription', \n",
    "                               'duration', 'sampling_rate', 'source']:\n",
    "                    # Only add simple types\n",
    "                    if isinstance(value, (str, int, float, bool)):\n",
    "                        metadata_record[key] = value\n",
    "            \n",
    "            metadata_records.append(metadata_record)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"\\n Error saving file {i}: {e}\")\n",
    "            \n",
    "            # Try saving with fallback filename\n",
    "            try:\n",
    "                safe_filename = f\"audio_{i:06d}.wav\"\n",
    "                safe_path = os.path.join(output_folder, safe_filename)\n",
    "                sf.write(safe_path, audio_array, sampling_rate)\n",
    "                \n",
    "                # Still save metadata with safe filename\n",
    "                metadata_record = {\n",
    "                    'filename': safe_filename,\n",
    "                    'sentence': transcription,\n",
    "                    'transcription': transcription,\n",
    "                    'source': source,\n",
    "                    'duration': round(len(audio_array) / sampling_rate, 3),\n",
    "                    'sampling_rate': sampling_rate,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "                metadata_records.append(metadata_record)\n",
    "                \n",
    "                print(f\"    Saved as fallback: {safe_filename}\")\n",
    "                saved_count += 1\n",
    "                error_count -= 1\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"    Fallback also failed: {fallback_error}\")\n",
    "    \n",
    "    # ============================== #\n",
    "    #     SAVE METADATA FILES        #\n",
    "    # ============================== #\n",
    "    \n",
    "    # Save CSV\n",
    "    if metadata_records:\n",
    "        print(f\"\\n Saving metadata to CSV...\")\n",
    "        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = metadata_records[0].keys()\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(metadata_records)\n",
    "        print(f\"    Saved: {csv_path}\")\n",
    "        \n",
    "      \n",
    "    # ============================== #\n",
    "    #     SUMMARY STATISTICS         #\n",
    "    # ============================== #\n",
    "    \n",
    "    print(f\"\\n Summary for '{split_name}':\")\n",
    "    print(f\"   Successfully saved: {saved_count} files\")\n",
    "    print(f\"    Errors: {error_count} files\")\n",
    "    print(f\"    Audio location: {output_folder}\")\n",
    "    print(f\"    Metadata CSV: {csv_path}\")\n",
    "    print(f\"    Metadata JSONL: {jsonl_path}\")\n",
    "    \n",
    "    if metadata_records:\n",
    "        # Calculate statistics\n",
    "        total_duration = sum(r['duration'] for r in metadata_records)\n",
    "        avg_duration = total_duration / len(metadata_records)\n",
    "        \n",
    "        print(f\"\\n Statistics:\")\n",
    "        print(f\"   Total duration: {total_duration/3600:.2f} hours\")\n",
    "        print(f\"   Average duration: {avg_duration:.2f} seconds\")\n",
    "        print(f\"   Total files: {len(metadata_records)}\")\n",
    "        \n",
    "        # Show sample transcriptions\n",
    "        print(f\"\\n Sample transcriptions:\")\n",
    "        for i, record in enumerate(metadata_records[:5]):\n",
    "            print(f\"   {i+1}. [{record['source']}] {record['filename']}\")\n",
    "            trans = record['sentence'][:100]\n",
    "            print(f\"      \\\"{trans}{'...' if len(record['sentence']) > 100 else ''}\\\"\")\n",
    "        \n",
    "        # Count by source\n",
    "        from collections import Counter\n",
    "        source_counts = Counter(r['source'] for r in metadata_records)\n",
    "        print(f\"\\n Distribution by source:\")\n",
    "        for source, count in source_counts.most_common():\n",
    "            print(f\"   {source}: {count} files\")\n",
    "    \n",
    "    return saved_count, error_count, metadata_records\n",
    "\n",
    "\n",
    "# ============================== #\n",
    "#     PROCESS ALL SPLITS         #\n",
    "# ============================== #\n",
    "\n",
    "# Save the 'train' split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "train_saved, train_errors, train_metadata = save_split_to_folder(\"train\", train_dir)\n",
    "\n",
    "# Save the 'test' split (renamed to 'dev')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "dev_saved, dev_errors, dev_metadata = save_split_to_folder(\"test\", dev_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7805b65c-8c20-42f0-bee4-15c9f9dd671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV in Python\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('audio_galsenai/train/metadata.csv')\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98a986-c6cc-4298-a403-4ec51f945711",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color: #FFFFFF; background-color: #8B4000; padding: 10px; border-radius: 5px;\">\n",
    "  Download kallama audio file  and reviewed annotation by clad team from GCP bucket\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bef7f8-7aa9-4e57-896a-19394953ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-storage tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe2defe-8acd-47a6-b4dc-79a7d607d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"powerful-bounty-463513-j4-e2ab92933732.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74cae5-cbde-452a-abea-f89057766ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### download aud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e738147-9ee1-4901-a035-acc3156d848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================== #\n",
    "#     CONFIGURATION              #\n",
    "# ============================== #\n",
    "\n",
    "# Google Cloud Storage bucket name\n",
    "BUCKET_NAME = 'unchecked-audio-data'\n",
    "\n",
    "# Local directory to save downloaded files\n",
    "LOCAL_BASE_DIR = 'downloaded_wolof_audio'\n",
    "\n",
    "\n",
    "TARGET_PREFIXES = [f'wolof-batch{i}/' for i in range(2, 9)]\n",
    "# This will download: wolof-batch2, wolof-batch3, ... wolof-batch8\n",
    "\n",
    "# Download entire wolof-audio folder \n",
    "# TARGET_PREFIXES = ['wolof-audio/']\n",
    "\n",
    "MAX_WORKERS = 16\n",
    "\n",
    "# ============================== #\n",
    "#     LOGGING SETUP              #\n",
    "# ============================== #\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='download_log.log', \n",
    "    level=logging.ERROR, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# ============================== #\n",
    "#     DOWNLOAD FUNCTION          #\n",
    "# ============================== #\n",
    "\n",
    "def download_blob(blob, local_folder):\n",
    "    \"\"\"\n",
    "    Downloads a single blob to the local folder structure.\n",
    "    \n",
    "    Args:\n",
    "        blob: Google Cloud Storage blob object\n",
    "        local_folder: Local base directory for downloads\n",
    "        \n",
    "    Returns:\n",
    "        True if downloaded successfully, False if failed, None if skipped\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create local path (preserves GCS folder structure)\n",
    "        local_path = os.path.join(local_folder, blob.name)\n",
    "        \n",
    "        # Create parent directories if they don't exist\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        \n",
    "        # Skip if file already exists and has correct size\n",
    "        if os.path.exists(local_path):\n",
    "            if os.path.getsize(local_path) == blob.size:\n",
    "                return None  # Already downloaded\n",
    "        \n",
    "        # Download the file\n",
    "        blob.download_to_filename(local_path)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download {blob.name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# ============================== #\n",
    "#     MAIN FUNCTION              #\n",
    "# ============================== #\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to download files from Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Google Cloud Storage Downloader\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(BUCKET_NAME)\n",
    "        print(f\" Connected to bucket: {BUCKET_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to connect to bucket: {e}\")\n",
    "        print(\"\\nMake sure you have:\")\n",
    "        print(\"1. Installed google-cloud-storage: pip install google-cloud-storage\")\n",
    "        print(\"2. Set GOOGLE_APPLICATION_CREDENTIALS environment variable\")\n",
    "        print(\"3. Correct bucket name in BUCKET_NAME variable\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Target prefixes: {TARGET_PREFIXES}\")\n",
    "    print(f\" Local directory: {os.path.abspath(LOCAL_BASE_DIR)}\")\n",
    "    print(f\" Max parallel threads: {MAX_WORKERS}\")\n",
    "    \n",
    "    # ============================== #\n",
    "    #     LIST ALL FILES             #\n",
    "    # ============================== #\n",
    "    \n",
    "    print(\"\\n Listing files from bucket... (this might take a moment)\")\n",
    "    blobs_to_download = []\n",
    "    \n",
    "    for prefix in TARGET_PREFIXES:\n",
    "        try:\n",
    "            blobs = list(bucket.list_blobs(prefix=prefix))\n",
    "            blobs_to_download.extend(blobs)\n",
    "            print(f\"   Found {len(blobs)} files in '{prefix}'\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to list blobs for prefix '{prefix}': {e}\")\n",
    "            print(f\" Error listing files for '{prefix}': {e}\")\n",
    "    \n",
    "    total_files = len(blobs_to_download)\n",
    "    \n",
    "    if total_files == 0:\n",
    "        print(\"\\n  No files found!\")\n",
    "        print(\"\\nPossible issues:\")\n",
    "        print(\"1. Check if TARGET_PREFIXES are correct\")\n",
    "        print(\"2. Verify bucket name is correct\")\n",
    "        print(\"3. Ensure you have read permissions\")\n",
    "        print(\"\\nTip: List bucket contents with:\")\n",
    "        print(\"   gsutil ls gs://unchecked-audio-data/\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n Found {total_files} total files\")\n",
    "    \n",
    "    # Calculate total size\n",
    "    total_size = sum(blob.size for blob in blobs_to_download)\n",
    "    total_size_gb = total_size / (1024**3)\n",
    "    print(f\" Total size: {total_size_gb:.2f} GB\")\n",
    "    \n",
    "    # ============================== #\n",
    "    #     DOWNLOAD FILES             #\n",
    "    # ============================== #\n",
    "    \n",
    "    print(f\"\\n Starting download with {MAX_WORKERS} parallel threads...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    downloaded = 0\n",
    "    skipped = 0\n",
    "    failed = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Use tqdm for progress bar\n",
    "        results = list(tqdm(\n",
    "            executor.map(lambda blob: download_blob(blob, LOCAL_BASE_DIR), blobs_to_download),\n",
    "            total=total_files,\n",
    "            unit=\"file\",\n",
    "            desc=\"Downloading\"\n",
    "        ))\n",
    "        \n",
    "        # Count results\n",
    "        for result in results:\n",
    "            if result is True:\n",
    "                downloaded += 1\n",
    "            elif result is None:\n",
    "                skipped += 1\n",
    "            elif result is False:\n",
    "                failed += 1\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d23c13-5bee-4c63-bc4d-0d7cd023d82f",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center; color: #FFFFFF; background-color: #8B4000; padding: 10px; border-radius: 5px;\">\n",
    "  checked trancription metadata\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91250e-c5d8-4853-8832-30a94611fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import logging\n",
    "from google.cloud import storage\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BUCKET_NAME = 'checked-audio-data' \n",
    "OUTPUT_CSV = 'dataset_annotations.csv'\n",
    "MAX_WORKERS = 16 \n",
    "\n",
    "\n",
    "TARGET_PREFIXES = [\n",
    "    'wolof-audio/', \n",
    "    'wolof-batch2/', 'wolof-batch3/', 'wolof-batch4/',\n",
    "    'wolof-batch5/', 'wolof-batch6/', 'wolof-batch7/', 'wolof-batch8/',\n",
    "   \n",
    "]\n",
    "\n",
    "# --- LOGGING ---\n",
    "logging.basicConfig(filename='extraction_errors.log', level=logging.ERROR)\n",
    "\n",
    "def extract_info_from_blob(blob):\n",
    "    \"\"\"\n",
    "    Tente de lire un blob comme un JSON, peu importe son extension.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Si c'est un dossier, on ignore\n",
    "        if blob.name.endswith('/'):\n",
    "            return None\n",
    "\n",
    "        # On t√©l√©charge le contenu en texte\n",
    "        json_content = blob.download_as_text()\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(json_content)\n",
    "        except json.JSONDecodeError:\n",
    "            # Ce n'est pas un JSON (peut-√™tre un fichier log ou autre), on ignore silencieusement\n",
    "            return None\n",
    "\n",
    "        # --- 1. FILTRE DE QUALIT√â ---\n",
    "        # On ne garde que les annotations accept√©es/valid√©es\n",
    "        last_action = data.get('last_action')\n",
    "        if last_action != 'accepted':\n",
    "             return None \n",
    "\n",
    "        # --- 2. R√âCUP√âRATION DE L'AUDIO ---\n",
    "        audio_url = None\n",
    "        if 'task' in data and 'data' in data['task']:\n",
    "            audio_url = data['task']['data'].get('audio') or data['task']['data'].get('url')\n",
    "        elif 'data' in data:\n",
    "            audio_url = data['data'].get('audio') or data['data'].get('url')\n",
    "            \n",
    "        # Fallback sur le nom de fichier si l'URL est manquante\n",
    "        if not audio_url:\n",
    "            audio_url = f\"gs://{blob.bucket.name}/{blob.name}\"\n",
    "\n",
    "        # --- 3. R√âCUP√âRATION DE LA TRANSCRIPTION ---\n",
    "        transcription = None\n",
    "        results = data.get('result', [])\n",
    "        \n",
    "        # Gestion des formats imbriqu√©s\n",
    "        if not results and 'annotations' in data:\n",
    "             results = data['annotations'][-1].get('result', [])\n",
    "\n",
    "        for res in results:\n",
    "            if 'value' in res and 'text' in res['value']:\n",
    "                transcription = res['value']['text'][0]\n",
    "                break\n",
    "\n",
    "        if transcription and audio_url:\n",
    "            return {'path': audio_url, 'text': transcription}\n",
    "        \n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        # On log l'erreur sans bloquer le script\n",
    "        logging.error(f\"Erreur sur {blob.name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "    print(f\"üì° Connexion au bucket : {BUCKET_NAME}\")\n",
    "    \n",
    "    unique_blobs = []\n",
    "    seen_blob_names = set()\n",
    "\n",
    "    print(\"Listing des fichiers (sans filtre d'extension)...\")\n",
    "    \n",
    "    # Si TARGET_PREFIXES est vide, on scanne tout le bucket\n",
    "    prefixes_to_scan = TARGET_PREFIXES if TARGET_PREFIXES else [None]\n",
    "\n",
    "    for prefix in prefixes_to_scan:\n",
    "        blobs = bucket.list_blobs(prefix=prefix)\n",
    "        for b in blobs:\n",
    "            # On prend tout ce qui n'est pas d√©j√† vu\n",
    "            if b.name not in seen_blob_names:\n",
    "                unique_blobs.append(b)\n",
    "                seen_blob_names.add(b.name)\n",
    "\n",
    "    print(f\" {len(unique_blobs)} fichiers potentiels trouv√©s. Analyse en cours...\")\n",
    "\n",
    "    valid_rows = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_blob = {executor.submit(extract_info_from_blob, blob): blob for blob in unique_blobs}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_blob), total=len(unique_blobs)):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                valid_rows.append(result)\n",
    "\n",
    "    print(f\" √âcriture du CSV : {OUTPUT_CSV}\")\n",
    "    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['path', 'text'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(valid_rows)\n",
    "\n",
    "    print(f\" Termin√© ! {len(valid_rows)} segments valid√©s r√©cup√©r√©s.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d7c4e-860b-4235-ba05-4ba6024be779",
   "metadata": {},
   "source": [
    " <div style=\"text-align: center; color: #FFFFFF; background-color: #8B4000; padding: 10px; border-radius: 5px;\">\n",
    "  get audio file from subfolder\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca1c8c-3012-42b1-b943-35c448a29614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "BASE_DIR = \"downloaded_wolof_audio\"\n",
    "SUBFOLDERS = ['wolof-audio'] + [f'wolof-batch{i}' for i in range(2, 9)]\n",
    "DEST_DIR = 'data'\n",
    "\n",
    "SOURCE_FOLDERS = [os.path.join(BASE_DIR, folder) for folder in SUBFOLDERS]\n",
    "\n",
    "\n",
    "print(SOURCE_FOLDERS)\n",
    "def move_wav_files():\n",
    "    # 1. Create the destination folder\n",
    "    if not os.path.exists(DEST_DIR):\n",
    "        os.makedirs(DEST_DIR)\n",
    "        print(f\" Created folder: {DEST_DIR}\")\n",
    "\n",
    "    count = 0\n",
    "    errors = 0\n",
    "\n",
    "    print(\" Starting move operation...\")\n",
    "\n",
    "    for folder in SOURCE_FOLDERS:\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\" Skipping missing folder: {folder}\")\n",
    "            continue\n",
    "\n",
    "        # Get list of files in this batch folder\n",
    "        files = os.listdir(folder)\n",
    "        \n",
    "        for filename in files:\n",
    "            # Filter for .wav files only\n",
    "            if filename.lower().endswith('.wav'):\n",
    "                src_path = os.path.join(folder, filename)\n",
    "                dst_path = os.path.join(DEST_DIR, filename)\n",
    "\n",
    "                if os.path.exists(dst_path):\n",
    "                    base, ext = os.path.splitext(filename)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(os.path.join(DEST_DIR, f\"{base}_dup{counter}{ext}\")):\n",
    "                        counter += 1\n",
    "                    new_filename = f\"{base}_dup{counter}{ext}\"\n",
    "                    dst_path = os.path.join(DEST_DIR, new_filename)\n",
    "                    print(f\"   Note: Renamed duplicate {filename} -> {new_filename}\")\n",
    "\n",
    "                try:\n",
    "                    shutil.move(src_path, dst_path)\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\" Error moving {filename}: {e}\")\n",
    "                    errors += 1\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\" Moved {count} .wav files to '/{DEST_DIR}'\")\n",
    "    if errors > 0:\n",
    "        print(f\" {errors} files failed to move.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    move_wav_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6e484-8578-4e16-91d3-a0f5b920d491",
   "metadata": {},
   "source": [
    " <div style=\"text-align: center; color: #FFFFFF; background-color: #8B4000; padding: 10px; border-radius: 5px;\">\n",
    "  Prepare metadata\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4e3ec-5a75-48a4-bce1-c4107636c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  from clad \n",
    "train_clad = pd.read_csv(\"train/metadata.csv\")\n",
    "test_clad = pd.read_csv(\"test/metadata.csv\")\n",
    "train_clad[\"source\"] = \"clad_review\"\n",
    "test_clad[\"source\"] = \"clad_review\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4dc9a2-2bca-4841-893d-87d68f0663b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clad['file_name'] = train_clad['file_name'].apply(os.path.basename)\n",
    "test_clad['file_name'] = test_clad['file_name'].apply(os.path.basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07834202-0a81-4c7c-90f9-398095feac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  from galsenai\n",
    "train_galsenai = pd.read_csv(\"train_galsen_metadata.csv\",usecols=[\"file_name\",\"sentence\",\"source\"])\n",
    "test_galsenai = pd.read_csv(\"test_galsen_metadata.csv\",usecols=[\"file_name\",\"sentence\",\"source\"])\n",
    "train_galsenai = train_galsenai.rename(columns={'filename': 'file_name'})\n",
    "test_galsenai = test_galsenai.rename(columns={'filename': 'file_name'})\n",
    "train_galsenai.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10126c3e-313b-4702-923d-08ab6cfa1900",
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep metadata\n",
    "train_galsenai.to_csv(\"train_galsen_metadata.csv\", index=False)\n",
    "test_galsenai.to_csv(\"test_galsen_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64430cc4-f053-4a91-a0e8-032b22b75715",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  from kallama linguiste\n",
    "#train_kallama = pd.read_csv(\"audio_galsenai/train/metadata.csv\")\n",
    "#test_kallama = pd.read_csv(\"audio_galsenai/test/metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f3529-f866-4c2f-97cc-4e5447d1b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_train = pd.concat([train_clad,train_galsenai])\n",
    "global_test = pd.concat([test_clad,test_galsenai])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a90b6-9bb1-4735-b6d8-63cd5ce42bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_train = pd.read_csv(\"gobal_corpus/train/metadata.csv\")\n",
    "global_test =   pd.read_csv(\"gobal_corpus/dev/metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
